import os
import cv2
import torch
import numpy as np
import json
from PIL import Image
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
from collections import defaultdict, OrderedDict
import io
import base64
from fastapi import FastAPI, HTTPException, File, UploadFile, Response
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import redis
import traceback
import time
from typing import List, Optional, Dict, Any
from east_model import East

# ====== CONFIG ======
# C√°c ƒë∆∞·ªùng d·∫´n linh ho·∫°t ƒë·ªÉ t·∫£i model
east_model_paths = [
    "/app/epoch_90_checkpoint.pth.tar",
    "./epoch_90_checkpoint.pth.tar",
]

trocr_model_paths = [
    "/app/seq2seq_model_printed/checkpoint-166002",
    "./seq2seq_model_printed/checkpoint-166002",
]

trocr_model_name = "microsoft/trocr-base-printed"

# Tham s·ªë ph√°t hi·ªán vƒÉn b·∫£n
score_map_thresh = 0.6
nms_thres = 0.5
merge_lines_enabled = False
y_overlap_threshold = 0.5

# Tham s·ªë m·ªü r·ªông box - t·ªëi ∆∞u h√≥a cho nh·∫≠n d·∫°ng vƒÉn b·∫£n ch√≠nh x√°c h∆°n
box_expand_ratio = 0.03  # TƒÉng nh·∫π ƒë·ªÉ b·∫Øt vƒÉn b·∫£n t·ªët h∆°n
horizontal_expand_factor = 0.2  # TƒÉng ƒë·ªÉ b·∫Øt ƒë∆∞·ª£c c√°c t·ª´ ƒë·∫ßy ƒë·ªß h∆°n

# C·∫•u h√¨nh GPU
gpu_ids = [0] if torch.cuda.is_available() else []
means = [100, 100, 100]
MAX_BATCH_SIZE = 16
MAX_REGIONS = 300

# S·ª≠ d·ª•ng CUDA n·∫øu c√≥
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ƒêang s·ª≠ d·ª•ng thi·∫øt b·ªã: {device}")

# Kh·ªüi t·∫°o FastAPI
app = FastAPI(title="TrOCR Service")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# K·∫øt n·ªëi Redis
redis_host = os.environ.get("REDIS_HOST", "localhost")
redis_client = redis.Redis(host=redis_host, port=6379, db=0)

# ƒê·ªãnh nghƒ©a c√°c models
class RecognizeRequest(BaseModel):
    preprocessed_image_id: str

class RecognitionResult(BaseModel):
    text: str
    bbox: List[List[int]]
    region_id: int

class RecognizeResponse(BaseModel):
    result: List[RecognitionResult]

# Bi·∫øn to√†n c·ª•c cho models
east_model = None
processor = None
trocr_model = None

def resize_image(image, size=1024):
    """Resize ·∫£nh cho EAST model v·ªõi k√≠ch th∆∞·ªõc c·ªë ƒë·ªãnh h√¨nh vu√¥ng"""
    h, w = image.shape[:2]
    
    # S·ª≠ d·ª•ng k√≠ch th∆∞·ªõc vu√¥ng c·ªë ƒë·ªãnh ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh nh·∫•t qu√°n
    target_height = size
    target_width = size
    
    # Resize v·ªÅ k√≠ch th∆∞·ªõc c·ªë ƒë·ªãnh
    resized = cv2.resize(image, (target_width, target_height))
    
    # T√≠nh t·ª∑ l·ªá ƒë·ªÉ chuy·ªÉn ƒë·ªïi t·ªça ƒë·ªô ng∆∞·ª£c l·∫°i
    h_ratio = h / target_height
    w_ratio = w / target_width
    
    return resized, (h_ratio, w_ratio)

def get_boxes_with_scores(score, geo, score_thresh=score_map_thresh):
    """Tr√≠ch xu·∫•t boxes v√† scores t·ª´ ƒë·∫ßu ra c·ªßa EAST model"""
    score_map = score.detach()[0, 0].cpu().numpy()
    geo_map = geo.detach()[0].permute(1, 2, 0).cpu().numpy()
    
    yxs = np.argwhere(score_map > score_thresh)
    
    boxes = []
    scores = []
    
    for y, x in yxs:
        distances = geo_map[y, x, :4]  # top, right, bottom, left
        top, right, bottom, left = distances

        cx, cy = x * 4.0, y * 4.0  
        x0, y0 = int(cx - left), int(cy - top)
        x1, y1 = int(cx + right), int(cy + bottom)
        
        boxes.append([x0, y0, x1, y1])
        scores.append(score_map[y, x])
    
    return boxes, scores

def nms(boxes, scores, nms_threshold=nms_thres):
    """Non-Maximum Suppression ƒë·ªÉ lo·∫°i b·ªè c√°c box ch·ªìng l·∫•p"""
    if len(boxes) == 0:
        return []
    
    boxes = np.array(boxes)
    x0, y0, x1, y1 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
    areas = (x1 - x0) * (y1 - y0)
    order = np.argsort(scores)[::-1]
    
    keep = []
    while order.size > 0:
        i = order[0]
        keep.append(i)
        xx0 = np.maximum(x0[i], x0[order[1:]])
        yy0 = np.maximum(y0[i], y0[order[1:]])
        xx1 = np.minimum(x1[i], x1[order[1:]])
        yy1 = np.minimum(y1[i], y1[order[1:]])
        
        w = np.maximum(0.0, xx1 - xx0)
        h = np.maximum(0.0, yy1 - yy0)
        intersection = w * h
        iou = intersection / (areas[i] + areas[order[1:]] - intersection)
        inds = np.where(iou <= nms_threshold)[0]
        order = order[inds + 1]
    
    return keep

def expand_box(box, expand_ratio=box_expand_ratio, horizontal_factor=horizontal_expand_factor, 
              image_width=None, image_height=None):
    """M·ªü r·ªông bounding box theo t·ª∑ l·ªá ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh ƒë·ªìng th·ªùi gi·ªØ trong gi·ªõi h·∫°n ·∫£nh"""
    x0, y0, x1, y1 = box
    width = x1 - x0
    height = y1 - y0
    
    # √Åp d·ª•ng c√°c h·ªá s·ªë m·ªü r·ªông kh√°c nhau theo chi·ªÅu ngang v√† d·ªçc
    x_expand = width * expand_ratio * (1 + horizontal_factor)
    y_expand = height * expand_ratio
    
    new_x0 = max(0, int(x0 - x_expand))
    new_y0 = max(0, int(y0 - y_expand))
    new_x1 = int(x1 + x_expand)
    new_y1 = int(y1 + y_expand)
    
    # ƒê·∫£m b·∫£o box n·∫±m trong gi·ªõi h·∫°n ·∫£nh
    if image_width:
        new_x1 = min(new_x1, image_width)
    if image_height:
        new_y1 = min(new_y1, image_height)
    
    return [new_x0, new_y0, new_x1, new_y1]

def convert_to_polygon_bbox(box, image_width=None, image_height=None):
    """
    Chuy·ªÉn ƒë·ªïi bounding box t·ª´ ƒë·ªãnh d·∫°ng [x1, y1, x2, y2] sang ƒë·ªãnh d·∫°ng ƒëa gi√°c [[x1,y1], [x2,y1], [x2,y2], [x1,y2]]
    ph√π h·ª£p cho vi·ªác v·∫Ω v√† hi·ªÉn th·ªã tr√™n UI.
    
    Args:
        box: Bounding box ƒë·ªãnh d·∫°ng [x1, y1, x2, y2]
        image_width: Chi·ªÅu r·ªông c·ªßa ·∫£nh ƒë·ªÉ ƒë·∫£m b·∫£o box n·∫±m trong gi·ªõi h·∫°n
        image_height: Chi·ªÅu cao c·ªßa ·∫£nh ƒë·ªÉ ƒë·∫£m b·∫£o box n·∫±m trong gi·ªõi h·∫°n
        
    Returns:
        Bounding box ƒë·ªãnh d·∫°ng ƒëa gi√°c: [[x1,y1], [x2,y1], [x2,y2], [x1,y2]]
    """
    # ƒê·∫£m b·∫£o box c√≥ ƒë·ªãnh d·∫°ng ch√≠nh x√°c [x1, y1, x2, y2]
    if not (isinstance(box, list) and len(box) == 4):
        # N·∫øu ƒë·ªãnh d·∫°ng kh√¥ng ch√≠nh x√°c, tr·∫£ v·ªÅ m·ªôt h√¨nh ch·ªØ nh·∫≠t m·∫∑c ƒë·ªãnh
        print(f"C·∫£nh b√°o: ƒê·ªãnh d·∫°ng box kh√¥ng h·ª£p l·ªá: {box}, s·ª≠ d·ª•ng m·∫∑c ƒë·ªãnh")
        return [[0, 0], [100, 0], [100, 100], [0, 100]]
    
    x1, y1, x2, y2 = box
    
    # ƒê·∫£m b·∫£o t·ªça ƒë·ªô l√† s·ªë nguy√™n
    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
    
    # ƒê·∫£m b·∫£o t·ªça ƒë·ªô n·∫±m trong gi·ªõi h·∫°n ·∫£nh
    if image_width is not None:
        x1 = max(0, min(x1, image_width - 1))
        x2 = max(0, min(x2, image_width))
    
    if image_height is not None:
        y1 = max(0, min(y1, image_height - 1))
        y2 = max(0, min(y2, image_height))
    
    # T·∫°o ƒëa gi√°c theo th·ª© t·ª± ƒë·ªìng h·ªì b·∫Øt ƒë·∫ßu t·ª´ g√≥c tr√™n b√™n tr√°i
    polygon = [
        [int(x1), int(y1)],  # top-left
        [int(x2), int(y1)],  # top-right
        [int(x2), int(y2)],  # bottom-right
        [int(x1), int(y2)]   # bottom-left
    ]
    
    return polygon

def ocr_crop(image, box, processor, trocr_model):
    """Th·ª±c hi·ªán OCR tr√™n m·ªôt v√πng text ƒë∆∞·ª£c crop s·ª≠ d·ª•ng TrOCR"""
    expanded_box = expand_box(box, 
                             expand_ratio=box_expand_ratio, 
                             horizontal_factor=horizontal_expand_factor,
                             image_width=image.shape[1], 
                             image_height=image.shape[0])
    
    x0, y0, x1, y1 = expanded_box
    crop = image[y0:y1, x0:x1]
    
    # Ki·ªÉm tra crop tr·ªëng ho·∫∑c kh√¥ng h·ª£p l·ªá
    if crop.size == 0 or crop.shape[0] == 0 or crop.shape[1] == 0:
        return "", 0.0  # Tr·∫£ v·ªÅ text tr·ªëng v√† confidence b·∫±ng 0
    
    # Chuy·ªÉn ƒë·ªïi sang PIL v√† resize v·ªÅ k√≠ch th∆∞·ªõc ph√π h·ª£p cho TrOCR
    # B·∫£o to√†n t·ª∑ l·ªá khung h√¨nh t·ªët h∆°n cho nh·∫≠n d·∫°ng vƒÉn b·∫£n
    try:
        pil_img = Image.fromarray(crop)
        # T√≠nh to√°n k√≠ch th∆∞·ªõc b·∫£o to√†n t·ª∑ l·ªá khung h√¨nh
        aspect = crop.shape[1] / crop.shape[0]  # width / height
        if aspect > 4:  # Text r·∫•t r·ªông
            new_width = min(128, crop.shape[1])  # Gi·ªõi h·∫°n chi·ªÅu r·ªông cho vƒÉn b·∫£n r·∫•t r·ªông
            new_height = max(32, int(new_width / aspect))
        else:
            new_height = 32  # Chi·ªÅu cao c·ªë ƒë·ªãnh ƒë·ªÉ x·ª≠ l√Ω nh·∫•t qu√°n
            new_width = max(32, min(128, int(new_height * aspect)))  # B·∫£o to√†n t·ª∑ l·ªá nh∆∞ng gi·ªõi h·∫°n chi·ªÅu r·ªông t·ªëi ƒëa
            
        pil = pil_img.resize((new_width, new_height), resample=Image.BILINEAR)
        
        # X·ª≠ l√Ω ·∫£nh v·ªõi TrOCR
        inputs = processor(images=pil, return_tensors="pt").pixel_values.to(device)
        with torch.no_grad():
            outputs = trocr_model.generate(inputs)
        
        # Gi·∫£i m√£ text ƒë√£ d·ª± ƒëo√°n
        text = processor.batch_decode(outputs, skip_special_tokens=True)[0]
        
        # ∆Ø·ªõc t√≠nh ƒë·ªô tin c·∫≠y (s·ª≠ d·ª•ng ƒë·ªô d√†i nh∆∞ m·ªôt heuristic ƒë∆°n gi·∫£n)
        # C√≥ th·ªÉ th√™m c√°ch ƒëo ƒë·ªô tin c·∫≠y tinh vi h∆°n ·ªü ƒë√¢y
        confidence = min(0.9, max(0.5, len(text) / 30)) if text else 0.0
        
        return text, confidence
    except Exception as e:
        print(f"L·ªói trong qu√° tr√¨nh x·ª≠ l√Ω OCR: {e}")
        return "", 0.0

def detect_text_boxes(image, east_model, processor, ocr_model, 
                     score_thresh=score_map_thresh, 
                     nms_thresh=nms_thres,
                     merge_lines=merge_lines_enabled,
                     y_overlap_threshold=y_overlap_threshold,
                     return_resized_info=True):
    """
    Pipeline ho√†n ch·ªânh ƒë·ªÉ ph√°t hi·ªán vƒÉn b·∫£n v·ªõi EAST + OCR s·ª≠ d·ª•ng TrOCR
    
    Args:
        image: ·∫£nh RGB ƒë·∫ßu v√†o
        east_model: model EAST ƒë√£ load
        processor: TrOCR processor
        ocr_model: model TrOCR
        score_thresh: ng∆∞·ª°ng cho ƒëi·ªÉm tin c·∫≠y c·ªßa EAST
        nms_thresh: ng∆∞·ª°ng cho non-maximum suppression
        merge_lines: c√≥ g·ªôp c√°c box c√πng d√≤ng kh√¥ng
        y_overlap_threshold: ng∆∞·ª°ng ch·ªìng l·∫•p theo chi·ªÅu y ƒë·ªÉ x√°c ƒë·ªãnh c√°c box thu·ªôc c√πng d√≤ng
        return_resized_info: c√≥ tr·∫£ v·ªÅ th√¥ng tin ·∫£nh ƒë√£ resize kh√¥ng
        
    Returns:
        valid_boxes: c√°c box vƒÉn b·∫£n cu·ªëi c√πng
        all_texts: vƒÉn b·∫£n ƒë∆∞·ª£c nh·∫≠n d·∫°ng cho m·ªói box
        all_confidences: ƒëi·ªÉm tin c·∫≠y cho m·ªói d·ª± ƒëo√°n
        resized_info: th√¥ng tin v·ªÅ ·∫£nh ƒë√£ resize (n·∫øu return_resized_info=True)
    """
    # L∆∞u k√≠ch th∆∞·ªõc ·∫£nh g·ªëc
    orig_height, orig_width = image.shape[:2]
    resized, (h_ratio, w_ratio) = resize_image(image, size=1024)
    # Log th√¥ng tin k√≠ch th∆∞·ªõc
    print(f"Original image: {orig_width}x{orig_height}")
    print(f"Resize ratios: h_ratio={h_ratio}, w_ratio={w_ratio}")
    
    # Chu·∫©n b·ªã tensor cho EAST model
    tensor = torch.from_numpy(resized).permute(2, 0, 1).float().unsqueeze(0).to(device)
    
    # Ch·∫°y EAST model ƒë·ªÉ l·∫•y ƒëi·ªÉm s·ªë v√πng text v√† h√¨nh h·ªçc
    with torch.no_grad():
        score, geo = east_model(tensor)
    
    # Tr√≠ch xu·∫•t boxes v√† scores t·ª´ ƒë·∫ßu ra c·ªßa EAST
    boxes, scores = get_boxes_with_scores(score, geo, score_thresh)
    print(f"Found {len(boxes)} text boxes before NMS")
    
    # √Åp d·ª•ng non-maximum suppression
    if len(boxes) > 0:
        keep_indices = nms(boxes, scores, nms_thresh)
        nms_boxes = [boxes[i] for i in keep_indices]
        nms_scores = [scores[i] for i in keep_indices]
        print(f"After NMS: {len(nms_boxes)} text boxes")
    else:
        nms_boxes = []
        nms_scores = []
        print("No text boxes detected")
    
    # Chuy·ªÉn ƒë·ªïi boxes v·ªÅ kh√¥ng gian ·∫£nh ƒë√£ resize
    resized_boxes = []
    for box in nms_boxes:
        resized_box = np.array(box, dtype=np.int32)
        resized_boxes.append(resized_box)
    
    # Nh·∫≠n d·∫°ng vƒÉn b·∫£n trong m·ªói box
    all_texts = []
    all_confidences = []
    valid_boxes = []
    valid_resized_boxes = []  # L∆∞u l·∫°i boxes trong kh√¥ng gian ·∫£nh ƒë√£ resize
    
    print("üîç Performing OCR on detected boxes...")
    for i, box in enumerate(resized_boxes):
        # Tr√≠ch xu·∫•t vƒÉn b·∫£n t·ª´ box
        text, confidence = ocr_crop(resized, box, processor, ocr_model)
        
        if text.strip():
            all_texts.append(text)
            all_confidences.append(confidence)
            
            # Chuy·ªÉn ƒë·ªïi box t·ª´ kh√¥ng gian ·∫£nh ƒë√£ resize v·ªÅ kh√¥ng gian ·∫£nh g·ªëc
            x0, y0, x1, y1 = box
            
            # √Åp d·ª•ng t·ª∑ l·ªá ch√≠nh x√°c ƒë·ªÉ chuy·ªÉn ƒë·ªïi t·ªça ƒë·ªô v·ªÅ ·∫£nh g·ªëc
            orig_x0 = int(x0 * w_ratio)
            orig_y0 = int(y0 * h_ratio)
            orig_x1 = int(x1 * w_ratio)
            orig_y1 = int(y1 * h_ratio)
            
            # ƒê·∫£m b·∫£o t·ªça ƒë·ªô n·∫±m trong gi·ªõi h·∫°n ·∫£nh
            orig_x0 = max(0, min(orig_x0, orig_width - 1))
            orig_y0 = max(0, min(orig_y0, orig_height - 1))
            orig_x1 = max(0, min(orig_x1, orig_width))
            orig_y1 = max(0, min(orig_y1, orig_height))
            
            orig_box = [orig_x0, orig_y0, orig_x1, orig_y1]
            
            valid_boxes.append(orig_box)
            valid_resized_boxes.append(box)
            # print(f"Box {i+1}/{len(resized_boxes)}: \"{text}\"")
            # print(f"  Resized box: {box}")
            # print(f"  Original box: {orig_box}")
        else:
            print(f"Box {i+1}/{len(resized_boxes)}: Empty text, skipping")
    
    # Tr·∫£ v·ªÅ k·∫øt qu·∫£ v·ªõi ho·∫∑c kh√¥ng c√≥ th√¥ng tin resize
    if return_resized_info:
        resized_info = {
            "original_size": (orig_width, orig_height),
            "resize_ratios": (w_ratio, h_ratio),
            "resized_boxes": valid_resized_boxes
        }
        return valid_boxes, all_texts, all_confidences, resized_info
    else:
        return valid_boxes, all_texts, all_confidences

def ensure_rgb_image(image: np.ndarray) -> np.ndarray:
    """
    Ensure the image is in RGB format with 3 channels.
    
    Args:
        image (np.ndarray): Input image
    
    Returns:
        np.ndarray: RGB image with 3 channels
    """
    # N·∫øu ·∫£nh l√† grayscale
    if len(image.shape) == 2:
        return cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)
    
    # N·∫øu ·∫£nh c√≥ 4 k√™nh (RGBA)
    if image.shape[2] == 4:
        return cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)
    
    # N·∫øu ·∫£nh l√† BGR (OpenCV format)
    if image.shape[2] == 3:
        return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    
    raise ValueError(f"Unsupported image format with {image.shape[2]} channels")

def initialize_models():
    """Kh·ªüi t·∫°o EAST v√† TrOCR models v·ªõi x·ª≠ l√Ω ƒë∆∞·ªùng d·∫´n c·∫£i ti·∫øn v√† kh√¥i ph·ª•c l·ªói"""
    global east_model, processor, trocr_model
    
    try:
        # T·∫£i EAST model
        east_model_loaded = False
        for path in east_model_paths:
            if os.path.exists(path):
                try:
                    print(f"[INFO] ƒêang t·∫£i EAST model t·ª´ {path}")
                    east_model = East().to(device)
                    
                    checkpoint = torch.load(path, map_location=device)
                    if 'state_dict' in checkpoint:
                        # X·ª≠ l√Ω state dict v·ªõi ti·ªÅn t·ªë 'module.' (t·ª´ DataParallel)
                        new_state_dict = OrderedDict()
                        for k, v in checkpoint['state_dict'].items():
                            name = k.replace('module.', '')
                            new_state_dict[name] = v
                        east_model.load_state_dict(new_state_dict)
                    else:
                        east_model.load_state_dict(checkpoint)
                        
                    east_model.eval()
                    print(f"[INFO] EAST model ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng t·ª´ {path}")
                    east_model_loaded = True
                    break
                except Exception as e:
                    print(f"[ERROR] Kh√¥ng th·ªÉ t·∫£i EAST model t·ª´ {path}: {e}")
                    traceback.print_exc()
        
        if not east_model_loaded:
            print("[WARNING] Kh√¥ng th·ªÉ t·∫£i EAST model t·ª´ b·∫•t k·ª≥ ƒë∆∞·ªùng d·∫´n n√†o")
            return False
        
        # T·∫£i TrOCR model
        trocr_model_loaded = False
        for path in trocr_model_paths:
            if os.path.exists(path):
                try:
                    print(f"[INFO] ƒêang t·∫£i TrOCR model t·ª´ {path}")
                    processor = TrOCRProcessor.from_pretrained(trocr_model_name)
                    trocr_model = VisionEncoderDecoderModel.from_pretrained(path).to(device)
                    trocr_model.eval()
                    print(f"[INFO] TrOCR model ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng t·ª´ {path}")
                    trocr_model_loaded = True
                    break
                except Exception as e:
                    print(f"[ERROR] Kh√¥ng th·ªÉ t·∫£i TrOCR model t·ª´ {path}: {e}")
                    traceback.print_exc()
        
        if not trocr_model_loaded:
            # Quay l·∫°i t·∫£i t·ª´ Hugging Face
            try:
                print("[INFO] ƒêang t·∫£i model TrOCR m·∫∑c ƒë·ªãnh t·ª´ Hugging Face")
                processor = TrOCRProcessor.from_pretrained(trocr_model_name)
                trocr_model = VisionEncoderDecoderModel.from_pretrained(trocr_model_name).to(device)
                trocr_model.eval()
                print("[INFO] Model TrOCR m·∫∑c ƒë·ªãnh ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng")
                trocr_model_loaded = True
            except Exception as e:
                print(f"[ERROR] Kh√¥ng th·ªÉ t·∫£i model TrOCR m·∫∑c ƒë·ªãnh: {e}")
                traceback.print_exc()
                return False
        
        print(f"[INFO] T·∫•t c·∫£ models ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng. S·ª≠ d·ª•ng thi·∫øt b·ªã: {device}")
        return east_model_loaded and trocr_model_loaded
            
    except Exception as e:
        print(f"[ERROR] L·ªói khi kh·ªüi t·∫°o models: {str(e)}")
        traceback.print_exc()
        return False

# Kh·ªüi t·∫°o model
if not initialize_models():
    print("[ERROR] Kh√¥ng th·ªÉ kh·ªüi t·∫°o models, d·ªãch v·ª• c√≥ th·ªÉ kh√¥ng ho·∫°t ƒë·ªông ch√≠nh x√°c")

# C√°c endpoints
@app.get("/")
def read_root():
    return {"message": "TrOCR Service is running"}

@app.get("/health")
def health_check():
    return {"status": "ok", "service": "trocr"}

@app.post("/recognize")
async def recognize_text(request: RecognizeRequest):
    """Nh·∫≠n d·∫°ng vƒÉn b·∫£n trong ·∫£nh ƒë√£ ƒë∆∞·ª£c ti·ªÅn x·ª≠ l√Ω"""
    try:
        if processor is None or trocr_model is None or east_model is None:
            if not initialize_models():
                raise HTTPException(status_code=500, detail="OCR models not initialized properly")
                
        # L·∫•y ·∫£nh ƒë√£ qua ti·ªÅn x·ª≠ l√Ω t·ª´ Redis
        image_bytes = redis_client.get(f"preprocessed:{request.preprocessed_image_id}")
        if not image_bytes:
            raise HTTPException(status_code=404, detail="Preprocessed image not found")
        
        # Gi·∫£i m√£ ·∫£nh
        nparr = np.frombuffer(image_bytes, np.uint8)
        image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        
        if image is None:
            raise HTTPException(status_code=400, detail="Failed to decode image")
        
        # Chuy·ªÉn sang ƒë·ªãnh d·∫°ng RGB ƒë·ªÉ x·ª≠ l√Ω
        image_rgb = ensure_rgb_image(image)
        
        # Ph√°t hi·ªán v√† nh·∫≠n d·∫°ng vƒÉn b·∫£n
        boxes, texts, confidences, resized_info = detect_text_boxes(
            image_rgb, east_model, processor, trocr_model,
            score_thresh=score_map_thresh,
            nms_thresh=nms_thres,
            return_resized_info=True
        )

        # Chuy·ªÉn ƒë·ªïi t·∫•t c·∫£ boxes sang ƒë·ªãnh d·∫°ng ƒëa gi√°c
        polygon_boxes = []
        for box in boxes:
            polygon = convert_to_polygon_bbox(
                box, 
                image_width=image_rgb.shape[1], 
                image_height=image_rgb.shape[0]
            )
            polygon_boxes.append(polygon)
        
        # T·∫°o k·∫øt qu·∫£ ph·∫£n h·ªìi
        ocr_results = []
        for i, (polygon, text, confidence) in enumerate(zip(polygon_boxes, texts, confidences)):
            ocr_results.append({
                        "text": text,
                        "confidence": float(confidence),
                        "bbox": polygon,
                        "region_id": i, 
                    })
        # return results

        # In ra c·∫•u tr√∫c k·∫øt qu·∫£ ƒë·ªÉ debug
        print("Return structure:")
        print(ocr_results)
        
        return {"result": ocr_results}
            
    except Exception as e:
        print(f"[ERROR] L·ªói khi nh·∫≠n d·∫°ng vƒÉn b·∫£n: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error recognizing text: {str(e)}")
    
if __name__ == "__main__":
    import uvicorn
    uvicorn.run("app:app", host="0.0.0.0", port=5002, reload=True)